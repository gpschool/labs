{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Extra: Deep Gaussian Processes\n",
    "### Gaussian Process Summer School 2020\n",
    "In this extra lab, we introduce practical use of _deep Gaussian processes_, expanding on the talk given this morning.\n",
    "\n",
    "The level of this notebook is aimed as an introduction to deep GPs, but will assume some familiarity of topics from labs 1 and 2, including regression and using inducing variables for scalable inference.\n",
    "\n",
    "This notebook makes use of [`PyDeepGP`](https://github.com/SheffieldML/PyDeepGP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started\n",
    "\n",
    "Make sure that you have the appropriate packages installed, including `PyDeepGP`: you can review the [Getting Started](https://gpss.cc/gpss20/getting_started/) page for detailed instructions.\n",
    "\n",
    "First, we will setup the notebook with libraries we are going to use. As in previous labs, we will use `GPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for maths\n",
    "import numpy as np\n",
    "# Plotting tools\n",
    "from matplotlib import pyplot as plt\n",
    "# we use the following for plotting figures in jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPy: Gaussian processes library\n",
    "import GPy\n",
    "\n",
    "# PyDeepGP: Deep Gaussian processes\n",
    "## if you cannot run deepgp make sure that it is installed by uncommenting\n",
    "# !pip install git+https://github.com/SheffieldML/PyDeepGP\n",
    "import deepgp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Gaussian Process Regression\n",
    "\n",
    "Recall, from Lab 2, the data for the winners of the Olympic men's marathon. We will use this again to highlight limitations with standard GPs and demonstrate how we can make use of deep GPs for such a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../.resources/olympic_marathon_men\", \"rb\") as fid:\n",
    "    data = pickle.load(fid)\n",
    "\n",
    "X, y = data['X'], data['Y'] # X represents the year, and y are the average pace of the winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our plotting environment\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot the training data, the results of the gold medal winning time for the Olympic men's marathon\n",
    "plt.plot(X, y, \"kx\", mew=2)\n",
    "\n",
    "# Annotate plot\n",
    "plt.legend(labels=[\"winning time\"]), plt.xlim((1890, 2018))\n",
    "plt.xlabel(\"year\"), plt.ylabel(\"pace (minutes per kilometer)\")\n",
    "plt.title(\"Olympic men's marathon gold medal winning times from 1896 to 2012\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you should be familiar with fitting a GP using `GPy`. We will fit a standard `RBF` kernel. Note that we are using a Gaussian likelihood, and have a constant mean for our GP (to be optimised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(\n",
    "    X, y,\n",
    "    mean_function=GPy.mappings.Constant(1,1),\n",
    "    kernel=GPy.kern.RBF(1)\n",
    ")\n",
    "\n",
    "m_full.optimize(messages=False)\n",
    "display(m_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is utility code from previous labs, adapted slightly so that it will work with deep GPs as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(model, X, training_points=False):\n",
    "    \"\"\" Plotting utility to plot a GP fit with 95% confidence interval \"\"\"\n",
    "    # Plot 95% confidence interval \n",
    "    m, _ = model.predict(X)\n",
    "    lci, uci = model.predict_quantiles(\n",
    "        X,\n",
    "        (2.5, 97.5)\n",
    "    )\n",
    "    plt.fill_between(X[:,0], lci[:,0], uci[:,0], alpha=0.5)\n",
    "    \n",
    "    # Plot GP mean and initial training points\n",
    "    plt.plot(X, m, \"-\")\n",
    "    plt.legend(labels=[\"GP fit\"])\n",
    "    \n",
    "    plt.xlabel(\"x\"), plt.ylabel(\"f\")\n",
    "    \n",
    "    # Plot training points if included\n",
    "    if training_points:\n",
    "        X_, Y_ = model.X, model.Y\n",
    "        plt.plot(X_, Y_, \"kx\", mew=2)\n",
    "        plt.legend(labels=[\"GP fit\", \"sample points\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that plotting the GP, we struggle to handle the outlier, giving an suboptimial fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = np.arange(1890,2019)[:, None] # predict winning pace every year between 1890 and 2018\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "# Plot the GP prediction of the latent function with training points\n",
    "plot_gp(m_full, Xnew, training_points=True)\n",
    "# Annotate plot\n",
    "plt.xlabel(\"year\"), plt.ylabel(\"pace (minutes per kilometer)\")\n",
    "plt.title(\"Exact GP fit of latent function $f$ describing marathon pace in min/km, assuming a Gaussian likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep GP Regression\n",
    "\n",
    "As we have seen before, the GP struggles to fit to the observations due to the presence of the outlier in 1904. Previously, in Lab 2, we modelled this with a non-Gaussian likelihood to give a heavy-tailed output using a Student's T-distribution. An alternative way to deal with the non-Gaussian is to construct a hierarchical model: a deep GP with two layers.\n",
    "\n",
    "$$\n",
    "    y = f_1(f_2(x)),\\quad \\mathrm{where}\\ f_1\\sim \\mathcal{GP}\\ \\mathrm{and}\\ f_2 \\sim \\mathcal{GP}\n",
    "$$\n",
    "\n",
    "We are essentially constructing the function, \n",
    "$$f : \\mathbf{x} \\underset{f_2}{\\mapsto} \\mathbf{z} \\underset{f_1}{\\mapsto}y,$$\n",
    "where $\\mathbf{z}$ is a latent variable in the hierarchical model. We can refer to $\\mathbf{z}$ as a 'layer' in a deep model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will no construct our deep GP using `pyDeepGP`. We have one hidden variable (or layer), and will initialise our latent representations with principle component analysis (PCA).\n",
    "\n",
    "For simplicity, we use the Gaussian RBF kernel for each layer, and have 1-D latent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 1\n",
    "latent_dim = 1\n",
    "\n",
    "kernels = [*[GPy.kern.RBF(latent_dim, ARD=True)]*num_hidden] # hidden kernels\n",
    "kernels.append(GPy.kern.RBF(X.shape[1])) # we append a kernel for the input layer\n",
    "\n",
    "m_deep = deepgp.DeepGP(\n",
    "    # this describes the shapes of the inputs and outputs of our latent GPs\n",
    "    [y.shape[1], *[latent_dim]*num_hidden, X.shape[1]],\n",
    "    X = X, # training input\n",
    "    Y = y, # training outout\n",
    "    inits = [*['PCA']*num_hidden, 'PCA'], # initialise layers\n",
    "    kernels = kernels,\n",
    "    num_inducing = 50,\n",
    "    back_constraint = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_deep.initialize_parameter()\n",
    "display(m_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising a Deep GP\n",
    "\n",
    "Reflecting on the defined model, we can see that we have $260$ parameters to optimise, including kernel and likelihood hyperparameters and inducing inputs and variables for each layer, as well as the latent parameters.\n",
    "\n",
    "You may find that running the optimisation as is will cause a collapse of the Gaussian noise in each layer, causing failure of the optimiser. In the next cell, we construct a utility function to reset the variance to $1.0$ to try and avoid this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_dgp(model, messages=True):\n",
    "    ''' Utility function for optimising deep GP by first\n",
    "        reinitiailising the Gaussian noise at each layer\n",
    "        (for reasons pertaining to stability)\n",
    "    '''\n",
    "    model.initialize_parameter()\n",
    "    for layer in model.layers:\n",
    "        layer.likelihood.variance.constrain_positive(warning=False)\n",
    "        layer.likelihood.variance = 1. # small variance may cause collapse\n",
    "    model.optimize(messages=messages, max_iters=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try optimising the deep GP. You may find that the optimisation fails with status `Errorb'ABNORMAL_TERMINATION_IN_LNSRCH`: just try rerunning the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimise_dgp(m_deep, messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our deep GP in the same fit as before, by calling `predict()` and `predict_quantiles()` to obtain a moments and quantiles from the optimised deep GP in the observation domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = np.arange(1890,2019)[:, None] # predict winning pace every year between 1890 and 2018\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# Plot the our deep GP prediction\n",
    "plot_gp(m_deep, Xnew, training_points=True)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"year\"), plt.ylabel(\"pace (minutes per kilometer)\")\n",
    "plt.title(\"Deep GP with %d hidden layer(s) fit describing marathon pace in min/km\" % num_hidden);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Mappings\n",
    "\n",
    "As we can see, the deep GP performs much better than the standard GP fit. We may also find it interesting to look at both the individual functions mapping between layers.\n",
    "\n",
    "We can visualise the latent mappings by propagating the input through layers -- since each output is multivariate Gaussian by virtue of the mappings being GPs, we can plot the uncertainty of the latent functions at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dgp_layers(model, X, training_points=True, axsize=(14,6)):\n",
    "    ''' Plot mappings between layers in a deep GP '''\n",
    "    if not isinstance(model, deepgp.DeepGP):\n",
    "        return plot_gp(model, X) # catch a batch GP\n",
    "\n",
    "    num_layers  = len(model.layers) # Get number of layers\n",
    "    layer_input = X # The first input\n",
    "    \n",
    "    _, axs = plt.subplots(num_layers, 1, figsize=(axsize[0], axsize[1]*num_layers))\n",
    "    \n",
    "    # The layers in a deep GP are ordered from observation to input,\n",
    "    # ... we will plot them from input to output, so reverse the layers\n",
    "    layers = list(reversed(model.layers))\n",
    "    for i, layer in enumerate(layers):\n",
    "        # We get the mean and confidence intervals of the layer\n",
    "        mu_i, _ = layer.predict(layer_input, include_likelihood=False)\n",
    "        lci_i, uci_i = layer.predict_quantiles(layer_input, (2.5, 97.5))\n",
    "        # Plot mean and 95% CI of latent function\n",
    "        axs[i].plot(layer_input, mu_i)\n",
    "        axs[i].fill_between(layer_input[:,0], lci_i[:,0], uci_i[:,0], alpha=0.5)\n",
    "        # Annotate plot\n",
    "        axs[i].set_ylabel(layer.name)\n",
    "        axs[i].set_xlabel(layers[i-1].name if i > 0 else \"input\")\n",
    "        axs[i].set_title(\"$f_%d$\" % (num_layers-i))\n",
    "        # Set mean as input for next layer\n",
    "        layer_input = np.linspace(*min(lci_i), *max(uci_i), num=100)[:,None]\n",
    "        \n",
    "        if training_points: # Plot propagated training points\n",
    "            axs[i].plot(\n",
    "                layer.X.mean.values if i > 0 else layer.X,\n",
    "                layer.Y.mean.values if i < num_layers-1 else layer.Y,\n",
    "                'kx', mew=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot our two GPs that form the hierarchical (deep) GP. The first maps the input domain to the first latent space, and the second maps from that latent space into space of our observation model.\n",
    "\n",
    "$$y = f_1(f_2(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_dgp_layers(m_deep, Xnew, training_points=True, axsize=(14, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "As with all models, we might want to sample from the posterior. We can do this with the mean and covariance of our GP layers, and use these to create random samples -- by the nature of the model, each sample from the first layer forms the input to the second. You can see the code to perform this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dgp(model, X, num_samples=1, include_likelihood=True):\n",
    "    ''' Sample from a hierarchical GP by propagating\n",
    "        samples through layers\n",
    "    '''\n",
    "    samples = []\n",
    "    jitter = 1e-5\n",
    "    count, num_tries = 0, 100\n",
    "    while len(samples) < num_samples:\n",
    "        next_input = X\n",
    "        if count > num_tries:\n",
    "            print(\"failed to sample\")\n",
    "            break\n",
    "        try:\n",
    "            count = count + 1\n",
    "            for layer in reversed(model.layers):\n",
    "                mu_k, sig_k = layer.predict(\n",
    "                    next_input, full_cov=True, include_likelihood=include_likelihood\n",
    "                )\n",
    "                sample_k = mu_k + np.linalg.cholesky(sig_k + jitter*np.eye(X.shape[0]))@np.random.randn(*X.shape)\n",
    "                next_input = sample_k\n",
    "            samples.append(sample_k)\n",
    "            count = 0\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return samples if num_samples > 1 else samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(2,1, figsize=(14,12))\n",
    "\n",
    "samples = sample_dgp(m_deep, Xnew, 10, include_likelihood=True)\n",
    "for sample in samples:\n",
    "    axs[0].plot(Xnew, sample, 'go', alpha=0.2)\n",
    "axs[0].plot(X, y, 'kx', mew=2)\n",
    "axs[0].set_title(\"observable samples\")\n",
    "\n",
    "samples = sample_dgp(m_deep, Xnew, 100, include_likelihood=False)\n",
    "for sample in samples:\n",
    "    axs[1].plot(Xnew, sample, 'g-', alpha=0.1)\n",
    "axs[1].plot(X, y, 'kx', mew=2)\n",
    "axs[1].set_title(\"latent samples\")\n",
    "\n",
    "[(ax.set_xlabel(\"year\"), ax.set_ylabel(\"pace (minutes per kilometer)\")) for ax in axs];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Now you have seen how we can fit a deep GP to data, and sample from the posterior. Using the following utility code snippets, construct a deep GP prior and same from it. Apply standard and deep GP regression to the model, and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approximating Discontinuities\n",
    "\n",
    "One of the benefits of deep Gaussian processes is that we can warp the latent GPs non-linearly, which helps model more complex structures, such as step discontinuties. In this section, we will show an example of how a deep GP can be used to model a rectangle function, and compare it to a standard GP.\n",
    "\n",
    "First, we will create some data from a rectangle function, with the upper and lower parts overlapping randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_rect(scale=1.):\n",
    "    ''' Create a rectangle function with overlap in upper and lower layers'''\n",
    "    x = np.linspace(0., 1., 400)\n",
    "    y = np.hstack([np.zeros(100), np.random.choice([0., 1.], size=(50,)), np.ones(100), np.random.choice([0., 1.], size=(50,)), np.zeros(100)])\n",
    "    return x[:,None], scale*y[:,None]\n",
    "\n",
    "# We generate and plot our data (not that there is no added noise)\n",
    "X, y = overlapping_rect()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(X, y, 'kx', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from previous labs, fitting a GP to this will be difficult because of the inherent smoothness assumed. However, we will fit and plot one all the same, just for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(X, y, kernel=GPy.kern.RBF(1))\n",
    "m_full.optimize()\n",
    "display(m_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = np.linspace(-0.2, 1.2, num=500)[:,None]\n",
    "plt.figure(figsize=(14,6))\n",
    "plot_gp(m_full, Xnew, training_points=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the GP struggles with the discontinuties and gives us an overly uncertain, and smooth fit.\n",
    "\n",
    "We will now try a simple 3-layer deep GP, $y = f_1(f_2(f_3(x)))$, with RBF kernels at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 2\n",
    "\n",
    "m_deep = deepgp.DeepGP(\n",
    "    [y.shape[1], *[1]*num_hidden, X.shape[1]],\n",
    "    X = X,\n",
    "    Y = y,\n",
    "    init = [*['PCA']*num_hidden, 'PCA'],\n",
    "    kernels = [*[GPy.kern.RBF(1, ARD=True)]*num_hidden, GPy.kern.RBF(1)],\n",
    "    num_inducing = 50,\n",
    "    back_constraint=False\n",
    ")\n",
    "optimise_dgp(m_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the fit now, and as we can see, the is close to the observations, with the greatest uncertainty occuring in the overlapping discontinuties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plot_gp(m_deep, Xnew, training_points=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the samples from the posterior, of both simulated observations and of the latent GPs. We can see that while the samples are not tightly fit to 0 or 1, we observe a reasonable behaviour representing the rectangle function (which we have treated as continuous). Notably, if we look to the edges of the plot, where we are sampling at input from which we have no observations, the behaviour is modelling the step functionality as a possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(2,1, figsize=(14,12))\n",
    "\n",
    "samples = sample_dgp(m_deep, Xnew, 10, include_likelihood=True)\n",
    "for sample in samples:\n",
    "    axs[0].plot(Xnew, sample, 'go', alpha=0.2)\n",
    "axs[0].plot(X, y, 'kx', mew=2)\n",
    "axs[0].set_title(\"observable samples\")\n",
    "\n",
    "samples = sample_dgp(m_deep, Xnew, 100, include_likelihood=False)\n",
    "for sample in samples:\n",
    "    axs[1].plot(Xnew, sample, 'g-', alpha=0.1)\n",
    "axs[1].plot(X, y, 'kx', mew=2)\n",
    "axs[1].set_title(\"latent samples\")\n",
    "\n",
    "[(ax.set_xlabel(\"x\"), ax.set_ylabel(\"y\")) for ax in axs];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot the different layers of the GP and comment on the warping. Can you explain why we are observing simulated discontinuities in the extrapolated regions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Credit\n",
    "\n",
    "This notebook was written by Wil Ward, based on documentation of PyDeepGP and talks by [Neil Lawrence](http://inverseprobability.com/). "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
